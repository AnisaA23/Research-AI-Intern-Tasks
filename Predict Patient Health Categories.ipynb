{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6009571a",
   "metadata": {},
   "source": [
    "Task 3: Predict Patient Health Categories "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513de97",
   "metadata": {},
   "source": [
    "1. For the first step of preprocessing the dataset, I imported beneficial libraries for this task and also assigned df as the variable that holds the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc00654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   # This import will be used for data manipulation\n",
    "import numpy as np   # This import will be used for working with arrays\n",
    "import seaborn as sns   # This import will be used for data visualisation and dataset loading \n",
    "import matplotlib.pyplot as plt   # This import will be used for plotting\n",
    "from sklearn.model_selection import train_test_split   # This import will be used for splittig data into train and test sets \n",
    "from sklearn.preprocessing import StandardScaler   # This import will be used for Scaling Features  \n",
    "\n",
    "df1 = pd.read_csv(\"test_data.csv\")   # Loads the dataset into the variable df1\n",
    "df = df1.drop(\"vitals_blood_pressure\", axis = 1)   # Due to blood pressure being displayed as systolic/diastolic this would alter the \n",
    "print(\"Dataset preview:\\n\", df.head())   # Displays the first few rows of th dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b413a1d",
   "metadata": {},
   "source": [
    "2. To be able to view some important statistics of the dataset, I used .info and .describe to be able to analyse the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c0bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will allow to view the basic information about the dataset\n",
    "print(df.info())   # Output: this will show the column names, data types, non-null counts and so on \n",
    "\n",
    "# We need to now also view the statistical summary of numrical columns \n",
    "print(df.describe())   # This wil output thingd such as the mean, std, min, 25%, 50%, 75%, max for numerical columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffab76",
   "metadata": {},
   "source": [
    "3. To be able to use numerical data only, any string data needs to be encoded first - in this case the sleep_quality column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2639bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the dataset contains string variables, I will encode sleep quality to integers\n",
    "df['sleep_quality'] = df['sleep_quality'].map({'poor':0, 'fair':1, 'good':2, 'excellent':3})   # This will map 'sleep_quality' to 0, 1, 2 and 3 for poor, fair, good and excellent \n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115acc1",
   "metadata": {},
   "source": [
    "4. For heart rate and temperature, below I calculated a mean to replace the list of values to be able to utilise the data better. First I had to replace the '|' symbol then work out the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seperate_string(val):   # Function to seperate the string and remove the '|' from the data points \n",
    "    try:\n",
    "        return np.mean([float(x) for x in val.split('|')])\n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "df['vitals_heart_rate'] = df['vitals_heart_rate'].apply(seperate_string)   # Replaces multiple data in the data point with the mean vital heart rate\n",
    "df['vitals_temperature'] = df['vitals_temperature'].apply(seperate_string)   # Replaces multiple data in the data point with the mean vital temperature\n",
    "\n",
    "print(\"\\nData with mean:\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58df2d",
   "metadata": {},
   "source": [
    "5. Now the data has only numerical values and has one value for each datapoint, the next step was to label the data based on thresholds. The three labels are:\n",
    " - Good: Stable vitals, adequate sleep, and healthy activity\n",
    " - Moderate: Minor deviations in health indicators\n",
    " - Poor: Significant abnormalities, insufficient sleep, or low activity\n",
    "\n",
    "Based on these labels, I created a point style system, to be able to accurately label each row of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_health(row):\n",
    "    points = 0\n",
    "    if row['vitals_heart_rate'] <= 70:   # if statement for heart rate to check health level \n",
    "        points += 2                        # Less than 70 means good health\n",
    "    elif row['vitals_heart_rate'] <= 75:\n",
    "        points += 1                        # Moderate health\n",
    "\n",
    "    if row['sleep_quality'] >= 2:   # if statement for sleep quality to check health level\n",
    "        points += 2                    # Quality of 2 or 3 (good or excellent) good health \n",
    "    elif row['sleep_quality'] == 1:\n",
    "        points += 1                          # Moderate health\n",
    "\n",
    "    if row['sleep_interruptions'] == 0:   # if statement for sleep interruptions to check health level\n",
    "        points += 2                          # No sleep interruptions good health\n",
    "    elif row['sleep_interruptions'] == 1:\n",
    "        points += 1                          # Moderate health\n",
    "    elif row['sleep_interruptions'] == 2:\n",
    "        points += 1\n",
    "\n",
    "    if row['activity_steps'] >= 6000:   # if statement for activity steps to check health level\n",
    "        points += 2                        # More than 6000 steps good health\n",
    "    elif row['activity_steps'] >= 4000:\n",
    "        points += 1                        # Moderate health\n",
    "\n",
    "    if points >= 8:\n",
    "        return 2    # Good health level\n",
    "    elif points >= 5:\n",
    "        return 1   # Moderate health level\n",
    "    else:\n",
    "        return 0   # Poor health level \n",
    "    \n",
    "df['health_label'] = df.apply(label_health, axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e9e508",
   "metadata": {},
   "source": [
    "6. To be able to prepare the data for machine learning models, I scaled features to be able to reduce abnormalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca22434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise scaler \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the following columns to achieve better accuracy when training models. \n",
    "df[['vitals_heart_rate', 'vitals_temperature','sleep_duration_hours', 'sleep_interruptions', 'activity_steps','activity_active_minutes', 'activity_sedentary_hours', 'nutrition_calories', 'nutrition_water_oz', 'nutrition_macros_carbs_g', 'nutrition_macros_protein_g', 'nutrition_macros_fat_g']] = scaler.fit_transform(df[['vitals_heart_rate', 'vitals_temperature', 'sleep_duration_hours', 'sleep_interruptions', 'activity_steps', \n",
    "    'activity_active_minutes', 'activity_sedentary_hours', 'nutrition_calories', 'nutrition_water_oz', 'nutrition_macros_carbs_g', 'nutrition_macros_protein_g', 'nutrition_macros_fat_g']])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d97897",
   "metadata": {},
   "source": [
    "7. Next was to set the data into training and testing sets to move onto training a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2387796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is split into features (X) and targets (y)\n",
    "\n",
    "# Feature variables to ensure the models can be as accurate as possible.\n",
    "X = df[[\"vitals_heart_rate\", \"sleep_duration_hours\", \"sleep_interruptions\", \"activity_steps\", \"activity_active_minutes\"]]   # All columns shown have been selected as features of this dataset.\n",
    "y = df[\"health_label\"]   # Target variable\n",
    "\n",
    "# Now we will split the data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Prints out the training and testing sets with the 70% and 30% split respectively.\n",
    "print(\"Training set size:\", X_train.shape) \n",
    "print(\"Testing set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5cdb2",
   "metadata": {},
   "source": [
    "8. Due to this being a classification task, I chose Logistic Regression as the model to train. Below is the layout of the report to showcase scores such as:\n",
    " - precision \n",
    " - recall\n",
    " - f1-score\n",
    " - support \n",
    " Once the model is trained, I will print out the classification report to show the outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af42cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report   #Imports for all important report metrics for classification tasks \n",
    "\n",
    "def print_score(clf, X_train, y_train, X_test, y_test, train = True):\n",
    "    if train:\n",
    "        pred = clf.predict(X_train)   # Here we will use predictions \n",
    "        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict = True, zero_division=0))   # For train reports \n",
    "        print(\"Train - Test Result:\\n====================================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n",
    "        print(\"____________________________________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")   # Shows values such as the F1-score, precision, recall and so on.\n",
    "\n",
    "    elif train == False:\n",
    "        pred = clf.predict(X_test)   # Here we will use prediction to gain information for the testing set \n",
    "        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict = True, zero_division=0))   # For test reports \n",
    "        print(\"Test - Test Result:\\n====================================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n",
    "        print(\"____________________________________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03c1c39",
   "metadata": {},
   "source": [
    "9. Below is the code that uses the training and testing sets to train the logistic regression model using an import from sklearn.linear_model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf892e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression   # Import to train the logistic regression model\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear')   # The liblinear library is used to optimise the models parameters during training \n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "print_score(lr_clf, X_train, y_train, X_test, y_test, train=True)   # Printing the outcome of the training set\n",
    "print()\n",
    "print()\n",
    "print_score(lr_clf, X_train, y_train, X_test, y_test, train=False)   # Printing the outcome of the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a62b92",
   "metadata": {},
   "source": [
    "10. To further show the outcome of the logistic regression model, below is the code to output the confusion matrix of the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1819514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(clf, X_train, y_train, X_test, y_test, train = True):   # Layout of the confusion matrix for visualisation \n",
    "    if train:   # Confusion matrix for training set.\n",
    "        pred = clf.predict(X_train)\n",
    "        print(\"____________________________________________________________________\")\n",
    "        print(f\"Confusion Matrix:\\n {confusion_matrix(y_train, pred)}\\n\")\n",
    "\n",
    "    elif train == False:   # Confusion matrix for testing set.\n",
    "        pred = clf.predict(X_test)\n",
    "        print(\"____________________________________________________________________\")\n",
    "        print(f\"Confusion Matrix:\\n {confusion_matrix(y_test, pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLogistic Regression:\")   # Prints out the logistic regression testing set confusion matrix\n",
    "print_score(lr_clf, X_train, y_train, X_test, y_test, train = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
